# 대규모 시스템 설계 2
[대규모 시스템 설계 2](https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=330404121)를 정리하여 만들었습니다.

Table of contents
=================
<!--ts-->
   * [분산 메세지 큐](#분산-메시지-큐)
   * [지표 모니터링 및 경보 시스템](#지표-모니터링-및-경보-시스템)
   * [광고 클릭 이벤트 집계](#광고-클릭-이벤트-집계)
   * [호텔 예약 시스템 설계](#호텔-예약-시스템-설계)
   * [분산 이메일 시스템 설계](#분산-이메일-시스템-설계)
   * [실시간 게임 순위표](#실시간-게임-순위표)
<!--te-->


분산 메세지 큐
=======
* `분산 메세지 큐 사용 시 이점`
  * 결합도 완화(decoupling): 컴포넌트 사이의 강한 결합이 사라지므로 각각 독립적으로 갱신가능 하여 규모 확장성과 가용성이 늘어남
  * 성능 개선: 비동기 통신을 쉽게 구현할 수 있으므로 성능이 향상됨
* `메시지 큐와 이벤트 스트리밍 플랫폼의 차이`
  * 전통적인 메시지 큐는 RabbitMQ가 있고 엄밀히 말하면 카프카는 `이벤트 스트리밍 플랫폼임`
  * 전통적인 메시지 큐는 메시지 보관 문제를 중요하게 다르지 않으며 일대일 메시지 전송 모델을 주로 사용하고 메시지 전달 순서도 보장하지 않아 경량적으로 가볍게 쓰기에는 좋음

* `분산 메시지 큐의 핵심 컴포넌트`
<img width="760" alt="스크린샷 2024-06-11 오전 3 14 45" src="https://github.com/yoonje/dev-note/assets/38535571/2ebd228e-be56-48a3-832f-53a2633c8e85">
  * 흐름
    * 생산자는 메세지를 메시지 큐에 발행
    * 소비자는 메시지 큐를 구독하고 구독한 메시지를 소비
    * 메시지 큐는 생산자와 소비사의 요청을 모두 받는 서버로서 사이에서 결합을 느슨하게 하여 생산자와 소비자의 독립성과 확장성을 보장
  * 메세지 모델
    * 일대일 모델: 전통적인 메시지 큐에선 일대일 모델을 사용하여 큐에 전송된 메시지는 오직 한 소비자만 가져갈 수 있으며 소비자가 메시지를 가져갔다는 사실이 알려지면 해당 메시지를 큐에서 삭제
    * 발행-구독 모델: 토픽을 통해 메시지를 주제별로 구분하고 이를 통해 구현
  * 생산자
    * 메시지를 특정 토픽을 전송 
  * 토픽, 파티션, 브로커, 오프셋
    * 토픽: 메시지를 주제 별로 보관하는 곳
    * 파티션: 토픽에 보낼 메세지의 주요 부분집합으로 파티션을 브로커에 분산하는 것으로 확장성을 보장하며 같은 파티션 안에서는 메시지 순서가 보장
    * 브로커: 카프카 애플리케이션이 설치된 서버
    * 오프셋: 파티션 내의 메시지의 위치
  * 소비자 그룹
    * 소비자 그룹 내 소비자는 토픽을 구독하여 메시지를 소비하기 위해 서로 협력하여 병렬로 데이터를 읽을 수 있음
    * 병렬로 읽을 경우 메시지 순서가 보장하기 위해서는 한 그룹 안에서는 오직 한 소비자만 읽을 수 있도록 설정
    * 모든 소비자를 같은 소비자 그룹에 두면 같은 파티션의 메시지는 오직 한 소비자만 가져갈 수 있어 일대일 모델이 됨
  * 조정 서비스
    * 서비스 탐색: 어떤 브로커가 살아 있는지 알려줌
    * 리더 선출: 브로커 가운데 컨트롤러 역할을 담당하여 파티션 배치를 책임지는 리더를 선출

* `분산 메시지 큐 상세 동작`
  * 메시지 데이터 저장소
    * 메시지 큐 데이터의 특징은 순차적인 읽기와 쓰기가 대부분이며 갱신 및 삭제는 발생하지 않는다는 것
    * 데이터베이스를 이용하는 경우 요구 사항을 충족할 순 있으나 설계가 어렵고 비용이 많이 드며 병목이 될 수 있음
    * `WAL(Write-Ahead Log)`는 새로운 항목이 추가되기만 하는 일반 파일인데 회전식 디스크를 사용한 파일 저장 방식도 순차적인 데이터 접근 패턴 시에는 읽기/쓰기 성능이 좋음
    * `WAL`에서 로그 파일 줄 번호를 오프셋으로 사용할 수 있는데 파일의 크기가 커질 수 있으므로 세그먼트로 단위로 나누어서 관리하며 새 메시지는 활성 상태의 세그먼트 파일에만 추가
  * 메시지 자료 구조
    * 키(key, byte[]): 파티션을 지정할 때 사용되며 키가 없는 경우 메시지의 파티션은 무작위적으로 결정하고 키가 있는 경우 `hash(key) % 파티션의 수`
    * 값(value, byte[]): 메시지의 내용인 페이로드를 의미
    * 토픽(topic, string): 메시지가 속한 토픽의 이름
    * 파티션(partition, integer): 메시지가 속한 파티션의 아이디
    * 오프셋(offset, long): 파티션 내 메시지의 위치
    * 타임스탬프(timestamp, long): 메시지가 저장된 시각
    * 크기(size, integer): 메시지의 크기
    * CRC(crc, integer): 순환 중복 검사

  * 메시지 일괄 처리
    * 네트워크 비용을 줄이기 위해 한번의 네트워크 요청으로 여러 개의 메시지를 전송할 수 있게 처리
    * 일괄 처리를 위해 생산자 측면에선 버퍼를 이용해 메시지를 보관했다가 전송할 수 있고 소비자 측면에선 오프셋부터 이벤트를 묶어 가져오는 방식으로 전송할 수 있음
    * 한번에 기록하면 큰 규모의 순차 쓰기 연산이 발생하여 디스크 캐시에서 더 큰 규모의 연속 공간 점유가 일어나서 높은 디스크 접근 대역폭 달성이 가능
    * 일괄 처리 메시지 양을 너무 높이면 `응답 지연`이 높아지므로 응답 지연이 중요한 서비스의 경우 일괄 처리 메시지 양을 줄이고 토픽 당 파티션의 수를 늘려서 처리량을 보장
  * 소비자 재조정
    * 소비자 재조정은 어떤 `소비자가 어떤 파티션을 책임지는지 다시 정하는 프로세스`로 소비자 그룹 안에 새로운 소비자가 합류하거나 기존 소비자가 그룹을 떠나거나 어떤 소비자에서 장애가 발생하거나 파티션이 재조정되는 경우 발생
    * 소비자 재조정을 위해 소비자들과 통신하는 브로커 노드인 코디네이터가 소비자로부터 오는 `heartbeat` 메시지를 살피고 파티션 내 오프셋 정보를 관리
  * 주키퍼 - 상태 저장소, 메타 데이터 저장소
    * 상태 저장소에는 소비자에 대한 파티션의 배치 관계와 오프셋 정보 등을 저장
    * 메타 데이터 저장소에는 토픽 설정이나 속성 정보 등을 저장
    * 상태 저장소와 메타 데이터 저장소에는 높은 일관성이 요구되며 데이터 양이 많지 않으므로 분산 시스템에서 주로 사용되는 계층적 키-값 저장소인 `주키퍼`를 주로 이용
  * 복제와 사본 동기화
    * 분산 시스템에서 높은 가용성을 보장하기 위해 복제를 진행하며 사본을 만들어 동기화를 진행하고 `특정 사본 개수`의 임게값을 넘으면 메시지를 잘 받았다는 응답을 통해 통신
    * 메세지는 사본 중에서 읽기와 쓰기를 담당하는 리더에게만 보내고 나머지 사본은 리더의 메시지를 가져와 동기화
    * `ISR(In-Sync Replica)` 상태는 `복제가 동기화 되는 것`을 일컫는 용어로 리더는 항상 ISR 상태이고 ISR 설정을 통해 성능과 영속성 사이의 타협을 진행
    * `ACK=all`은 모든 ISR이 메시지를 수신한 뒤에 ACK 응답을 받는 설정으로 성능은 떨어지지만 영속성 측면에서 가장 좋은 구성
    * `ACK=1`은 리더가 메시지를 저장하고 나면 ACK 응답을 받는 설정으로 적당한 응답 지연 가지고 있지만 리더 외 다른 사본에 메시지가 저장되진 않았으므로 메시지 전파 시점에 리더에 장애가 난 경우 데이터가 유실됨
    * `ACK=0`은 보낸 메시지에 대한 수신 확인 메시지를 기다리지 않고 계속 메시지를 전송하며 재시도를 하지 않는 설정으로 매우 낮은 응답 지연을 기지고 있지만 데이터 손실이 발생해도 상관이 없는 경우에만 사용
  * 규모 확장성
    * 생산자는 그룹 단위의 조정이 없기 때문에 새로운 생산자를 추가함으로써 확장 및 삭제가 가능
    * 소비자도 새로운 소비자 그룹은 쉽게 추가 및 삭제가 가능하나 `같은 소비자 그룹 내` 소비자가 추가되는 경우 `재조정 매커니즘`으로 처리해야함
    * 브로커가 추가되거나 삭제될 때 사본을 재배치하는 방법을 사용하거나 브로커가 추가되거나 삭제되는 경우 한시적으로 시스템에 설정된 사본 수보다 더 많은 사본을 허용하도록 하는 방법을 사용하여 확장 및 삭제가 가능
    * 파티션의 수가 조정되는 경우 생산자는 토픽을 보고 메시지를 던지고 브로커 단에서 저장이 일어나므로 크게 관련이 없고 소비자는 소비자 재조정을 통해 파티션 수 증가 및 삭제에 대응
  * 메시지 전달 방식
    * 최대 한번: 메시지를 최대 한번만 전달하는 방식으로 `ACK=0`을 통해서 구현하며 메시지를 처리하기 전에 오프셋부터 갱신하여 구현
    * 최소 한번: 메시지를 최소 한번만 전달하는 방식으로 `ACK=1` 또는 `ACK=all`을 통해서 구현하며 메시지를 처리한 뒤에만 오프셋을 갱신하여 구현
    * 정확히 한번: 메시지를 정확히 한번만 전달하는 방식으로 가장 까다로운 전송 방식으로 금융 시스템에 적합하지만 성능이 매우 떨어지고 애플리케이션에서 별도 구현해야함


지표 모니터링 및 경보 시스템
=======
* `지표 모니터링 및 경보 시스템 기본 기능`
  * 데이터 수집
  * 데이터 전송
  * 데이터 저장소
  * 경보
  * 시각화
* `지표 모니터링 및 경보 시스템 설계`
<img width="777" alt="스크린샷 2024-06-19 오후 9 27 23" src="https://github.com/yoonje/dev-note/assets/38535571/18ea45f7-4303-4e27-8f6c-345cf1c4dd88">
  * `데이터 모델과 데이터 접근 패턴`
    * 지표 데이터는 통상 시계열 데이터 형태로 기록되어 값에 타임 스탬프가 붙은 형태
    * 지표 데이터에는 write 가 read 보다 압도적으로 많이 발생하고 읽기 부하는 일시적으로 치솟았다가 사라지는 경향
  * `데이터 저장소 시스템`
    * 관계형 데이터 베이스는 통상적으로 시계열 데이터를 처리하는데 최적화되어 있지 않고 NoSQL 데이터베이스를 활용하는 방안도 그다지 매력적이지 않음
    * 시계열 데이터를 전문으로 관리하는 시계열 데이터 베이스 중에서 InfluxDB와 프로메테우스가 대세
  * `지표 수집 모델`
    * 풀 모델: 애플리케이션에서 주기적으로 지표 데이터를 가져오는 지표 수집기로 가져오는 흐름으로 서비스 탐색를 활용해 어느 서버에게 데이터를 요청해서 가져올 지 알아내기 위한 로직 필요
    * 푸시 모델: 서버나 데이터베이스에서 직접 지표 수집기로 지표를 전송하는 흐름으로 로드밸런서를 활용해 지표 수집기의 수평 확장 로직 필요
  * `카프카를 통한 규모 확장`
    * 시계열 데이터베이스에 장애가 생기거나 지표 데이터의 처리량을 늘리고 쉬운 규모 확장을 위해 카프카를 사용
    * 대역폭 요구에 따라 파티션의 수를 조정할 수 있고 태그/레이블에 따라 지표 데이터를 세분화 하여 파티션을 관리할 수도 있으며 지표 이름에 따라 어떤 파티션에 배치할지 결정할 수 있음
  * `질의 서비스`
    * 질의 처리 전담 서비스를 두면 시계열 데이터 베이스와의 결합도를 낮출 수 있으나 질의어를 제공하는 시계열 데이터 베이스가 많기 때문에 불필요한 경우도 많음
    * 질의 결과를 저장하는 캐시 서버를 도입하면 질의 부하를 낮추고 서비스의 성능을 높일 수 있음
  * `경보 시스템과 시각화 시스템`
    * 상용 경보 시스템과 시각화 시스템이 시계열 데이터 베이스와의 연동 및 플러그인들을 많이 지원하므로 이를 충분히 활용

광고 클릭 이벤트 집계
=======
* `광고 클릭 이벤트 집계 기본 기능`
  * M분 동안의 특정 광고 클릭 수 집계
  * 매분 가장 많이 클릭된 상위 광고 아이디 반환
  * 다양한 속성에 따른 집계 필터링
  * 지연 시간 및 중복에 대한 적절한 처리
* `질의 API 설계`
  * 지난 M분간 각 ad_id에 발생한 클릭 수 집계 API - GET /v1/ads/{ad_id}/aggregated_count
     * from: 집계 시작 시간
     * to: 집계 종료 시간
     * filter: 필터링 전략
  * 지난 M분간 가장 클릭이 많이 발생한 상위 N개의 ad_id 목록 반환 API - GET /v1/ads/popular_ads
     * count: 상위 광고 개수
     * window: 분 단위로 표현된 집계 윈도우 크기
     * filter: 필터링 전략
* `광고 클릭 이벤트 집계 설계`
![image](https://github.com/yoonje/dev-note/assets/38535571/55fad984-ec51-479b-8fdf-2b778063153e)
  * 데이터 베이스
    * 원시 데이터는 대규모 쓰기 연산이 많이 발생하고 시간 범위 질의가 많이 발생하므로 InfluxDB나 카산드라 DB를 주로 사용
    * 집계 데이터는 본질적으로 시계열 데이터이며 쓰기 작업이 많이 이뤄지므로 InfluxDB나 카산드라 DB를 주로 사용
  * 비동기 처리와 전달 보장
    * 카프카같은 메시지 큐를 도입하여 생산자와 소비자 간의 결합을 끊어 특정 컴포넌트의 장애에 대비
    * 집계 서비스의 경우 `정확히 한번`을 구현하기 위해서 카프카 메시지가 필요
    * 첫번째 메시지 큐에는 광고 클릭 이벤트 데이터가 기록되고 두번째 메시지 큐에는 `분 단위로 집계된 광고 클릭 수`와 `분 단위로 집계한 가장 많이 클릭한 상위 N개 광고`가 기록
    * https://www.youtube.com/watch?v=7_VdIFH6M6Q&t=12s
  * `집계 서비스`
    * 맵리듀스 프레임워크(DAG)
    ![image](https://github.com/yoonje/dev-note/assets/38535571/f3fba188-0b02-440b-98d6-18f7a73c83f0)
      * 맵/집계/리듀스 노드 등으로 작은 컴퓨팅 단위로 세분화 하여 각 노드가 하나의 작업만 처리하고 다음 노드에 인계하여 처리
      * 맵리듀스 방식은 대규모 데이터를 분산 + 병렬 + 제대로 된 셔플 처리 할 때 효과가 있으나 싱글 스레드 환경에서는 맵리듀스를 써봤자 그냥 전통적인 방식으로 처리하는거랑 성능이 비슷
      * 맵 노드: 데이터 출처에서 읽은 데이터를 필터링하고 변환하는 역할
      * 집계 노드: 광고 아이디별 광고 클릭 이벤트 수를 매분 메모리에서 집계하는 역할
      * 리듀스 노드: 모든 집계 노드가 산출한 결과를 최종 결과로 축약
    * 스트리밍과 일괄 처리
      * 스트림 처리는 데이터를 오는 대로 처리하고 거의 실시간으로 집계된 결과를 생성하는데 사용하고 일괄 처리는 이력 데이터를 백업하기 위해 사용
      * 람다: 일괄 및 스트리밍 처리를 동시에 지원하는 시스템으로 두가지 처리 경로를 지원하므로 코드를 2벌 관리해야함
      * 카파 아키텍처: 일괄 처리와 스트리밍 처리 경로를 하나로 결합하여 해결하며 단일 스트림 처리 엔진을 사용하여 실시간 데이터 처리 및 데이터 재처리 문제를 같이 해결
    * 집계 윈도우
      * 텀블링 윈도우: 시간을 나누고 같은 크기의 겹치지 않는 구간으로 분할하여 처리하는 방식으로 매분 발생한 클릭 이벤트를 집계하기에 적합
      * 슬라이딩 윈도: 데이터 스트림을 미끄러져 나아가며너 같은 시간 구간 안에 있는 이벤트를 집계하는 방법으로 M 분간 가장 많이 클릭된 상위 광고 개수를 알아내기에 적합
    * 시간 처리
      * 광고 클릭이 발생한 시각과 집계 서버가 클릭 이벤트를 처리한 시스템의 시각 중에서 데이터 정확도를 위해서 클릭 이벤트가 발생한 시각을 사용
      * 이벤트가 발생한 시각을 집계에 사용하는 경우 지연된 이벤트 처리 문제를 해결해야며 늦게 도착한 이벤트를 처리하기 위해 집계 윈도우의 확장인 `워터마크`를 사용
      * 워터마크를 크게 사용하면 데이터의 정확도는 높아지지만 대기 시잔이 늘어 지연 시간이 늘어남
  * `시스템 규모 확장`
    * 메시지 큐 규모 확장
      * 생산자: 생산자 인스턴스 수에 제한을 두지 않으므로 쉽게 확장 가능
      * 소비자: 소비자 그룹 내 재조정 매커니즘은 노드 추가/삭제를 통해 규모를 쉽게 조정
      * 브로커: 광고 아이디를 해시키로 사용하여 같은 카프카 파티션에 저장하거나 토픽을 나눠서 시스템 대역폭 처리를 늘릴 수 있음
    * 집계 서비스 규모 확장
      * 집계 서비스의 핵심인 맵리듀스 연산을 광고 아이디 마다 별도의 처리 스레드를 둬서 처리하거나 집계 서비스 노드를 아파치 하둡 YARN과 같은 자원 공급자에 배포하여 다중 프로세싱을 활용
    * 데이터 베이스 규모 확장
      * InfluxDB나 카산드라 DB는 안정 해시를 통해 수평적 규모 확장 기능을 지원
  * `데이터 모니터링`
    * 지연 시간: 데이터를 처리하는 각 단계마다 지연 시간이 추가될 수 있으므로 시스템의 중요 부분마다 시각 추적을 가능하도록 하여 모니터링
    * 메시지 큐 크기: 큐의 크기가 늘어나면 집계 노드가 더 많이 필요할 수 있으므로 카프카의 `Lag` 지표를 통해 추적
    * 집계 노드의 시스템 자원: 집계 노드의 CPU, Disk, Memory 등등 모니터링

호텔 예약 시스템 설계
=======
* `호텔 예약 시스템 개략 설계`
  * API 설계
    * 로그인 사용자의 예약 이력 반환
    * 특정 예약의 상세 정보 반환
    * 신규 예약
    * 예약 취소
  * 데이터 모델
    * 예약을 제외하고는 읽기 연산이 대부분이므로 RDBMS를 선택
    * RDBMS는 ACID를 잘 보장하여 분산 환경에서도 예약 시스템을 구축하기 편리하고, 데이터 모델링도 간편
  * MSA 아키텍처
    * 호텔 예약 시스템은 간단한 서비스들이 나누어져 있으므로 마이크로 서비스 아키텍처를 사용
    * 호텔 서비스, 요금 서비스, 예약 서비스, 결제 서비스, 호텔 관리 서비스
* `호텔 예약 시스템 상세 설계`
  * 데이터 모델
  * 동시성 문제 - 같은 사용자가 여러 예약 요청
    * 멱등 API를 사용해 예약을 하여 몇 번을 호출하더라도 같은 결과를 내는 멱등 API를 통해 이중 예약 문제를 해결
    * 예약 서비스에서 특정 상황에 대해 키를 부여하고 사용자가 이 키를 저장하여 사용
  * 동시성 문제 - 여러 사용자가 같은 객식을 동시에 예약 요청
    * 비관적 락: 비관적 락을 사용하면 사용자가 레코드를 갱신하려는 순간 즉시 데이터베이스에 락을 걸어 동시 업데이트를 방지하는 기술로 다른 사용자의 요청은 해당 요청이 종료될 때까지 진행이 되지 않음
    * 낙관적 락: 낙관적 락은 여러 사용자가 동시에 같은 자원에 접근하는 것이 허용되며 어플리케이션 단에서도 버저닝을 통해 제어
    * 데이터베이스 제약 조건: 데이터베이스 제약 조건은 낙관적 락과 유사하며, 데이터 베이스에서 total - reserved가 음수가 되는 순간 트랜잭션 롤백에 대한 규칙을 넣는 것
  * 시스템 규모 확장
    * 데이터베이스 샤딩: hotel의 아이디로 구분되기 때문에 해당 키를 샤딩 조건으로 사용
    * 캐시: 예약이 되지 않은 어제의 객실 데이터는 스케줄링을 통해 삭제하여도 무방하여 데이터베이스 앞에 캐시 계층을 두어 잔여 객실 확인 및 객실 예약 로직이 해당 계층에서 실행
  * 마이크로 서비스 독자적인 데이터베이스 일관성 문제
    * 2단계 커밋(2PC)
      * 여러 노드에 걸친 원자적 트랜잭션 실행을 보증하는 데이터베이스 프로토콜로, 모든 노드가 성공하지 않으면 실패하게 된다
      * 어느 한 노드에 장애가 발생하면 해당 장애가 복구될 때까지 진행이 중단된다
      * 여러 노드에 걸친 하나의 트랜잭션을 통해 ACID 속성을 만족시킨다
    * SAGA
      * 각 노드에 국지적으로 발생하는 트랜잭션을 하나로 엮은 것으로, 각각의 트랜잭션이 완료되면 다음 트랜잭션을 시작하는 트리거를 메시지로 보낸다. 한 트랜잭션이라도 실패하면 그 이전 트랜잭션의 결과를 전부 되돌리는 트랜잭션들을 실행한다
      * 각 단계가 하나의 트랜잭션이라 결과적 일관성에 의존한다


분산 이메일 시스템 설계
=======
* `분산 이메일 시스템 기본 기능`
  * 이메일 수신/발신
  * 읽음 여부에 따른 이메일 필터링
  * 제목, 발신인, 메일 내용에 따른 검색
  * 스팸 및 바이러스 방지 기능
  * HTTP를 사용한 연결
  * 첨부파일 지원
* `이메일 프로토콜`
  * SMTP: Simple Mail Transfer Protocol의 준말로, 이메일을 한 서버에서 다른 서버로 보내는 표준 프로토콜
  * POP: 이메일 클라이언트가 원격 메일 서버에서 이메일을 수신하고 다운로드 하기 위해 사용하는 표준 프로토콜
  * IMAP: Internet Mail Access Protocol의 준말로, 원격 메일 서버에서 이메일을 수신하는데 사용하는 또 다른 표준 프로토콜로 가장 많이 사용됨
  * HTTPS: 기술적으로 보자면 메일 전송 프로토콜은 아니나 웹 기반 이메일 시스템의 메일함 접속에 이용될 수 있음
* `DNS`
  * 수신자 도메인의 메일 교환기 레코드 검색에 이용
  * 우선순위가 높은 도메인이 먼저 사용되며 송신자 측 메일 서버는 이 메일 서버에 접속하여 메시지를 보내려고 시도
  * 연결에 실패하면 그다음 우선 순위가 높은 메일 서버에 연결을 시도
* `분산 이메일 시스템 개괄 설계`
  * 분산 메일 서버 아키텍처
    * 기본적으로 웹서버로 사용자가 이용하는 요청/응답 서비스 등의 API 요청을 처리하고, 실시간 서버(상태 유지 서버)를 웹소켓 혹은 롱폴링 등을 이용하며 새로운 이메일 내역을 전달
    * 웹소켓이 더 합리적이지만 브라우저 호환성 문제가 있어 롱 폴링도 고려
  * 메타데이터 데이터베이스
    * 이메일 제목, 본문, 발신인, 수신인 목록 등의 메타 데이터를 저장하는 데이터베이스
  * 첨부 파일 저장소
    * 정적 파일이므로 아마존 S3와 같은 객체 저장소를 사용
  * 분산 캐시
    * 최근에 수신된 이메일이 자주 읽을 가능성이 더 높으므로 클라이언트로 하여금 메모리에 캐시해 두도록 하면 메일을 표시하는 시간을 많이 줄일 수 있음
    * Redis를 주로 사용
  * 검색 저장소
    * 검색 저장소는 분산 문서 저장소로 고속 텍스트 검색을 지원하는 역 인덱스를 자료구조로 사용
    * ES를 주로 사용
* `분산 이메일 시스템 상세 설계`
  * 이메일 전송 절차
  ![image](https://github.com/yoonje/dev-note/assets/38535571/a9a6ea46-232b-46e1-a91a-061831f90053)
    * 웹 서버에서 기본적인 이메일을 검증하고 수신자 이메일 주소 도메인이 송신자 이메일 주소 도메인과 같은 경우 이메일 내용의 스팸 여부와 바이러스 감염 여부를 검사한 뒤 통과된 이메일은 송신자와 수신자의 편지함에 저장
    * 메시지 큐에 넣기에 첨부 파일의 크기가 너무 큰 이메일의 경우 객체 저장소에 따로 저장하고, 해당 저장 위치에 대한 참조 정보만 저장하고 기본적인 검증에 실패한 메일은 에러 큐에 담음
    * 외부 전송 담당 SMTP 작업 프로세스는 큐에서 메시지를 꺼내어 이메일의 스팸 및 바이러스 감염 여부를 확인하고 검증 절차를 통과한 이메일은 저장소 계층 내의 보낸 편지함에 저장
    * 외부 전송 담당 SMTP 작업 프로세스가 수신자의 메일 서버로 메일을 전송
    * 메시지 큐에 보관되는 모든 메시지에는 이메일을 생성하는 데 필요한 모든 메타 데이터가 포함되어 있으며 비동기적 메일 처리를 가능하게 하고 외부 메일 서버 전송용 SMTP 프로세스를 분리하여 독립적으로 규모를 조정
    * 메일이 처리되지 않고 큐에 오랫동안 남아있다면 원인을 분석하여 사후 처리
  * 이메일 수신 절차
  ![image](https://github.com/yoonje/dev-note/assets/38535571/cc298c18-d671-410e-9212-d31506186cd0)
    * 이메일이 SMTP 서버의 로드 밸런서에 도착해 분산된 SMTP 서버로 나뉨
    * 이메일의 첨부 파일이 큐에 들어가기 너무 큰 경우 첨부 파일 저장소(S3)에 보관
    * 이메일을 수신 이메일 큐에 넣어 결합도를 낮추며 수신되는 이메일의 양이 폭증하는 경우 버퍼 역할을 수행
    * 메일 처리 프로세스는 스팸 메일을 걸러내고, 바이러스를 차단하는 역할을 한 뒤 이메일을 메일 저장소, 캐시, 객체 저장소 등에 보관
  * 데이터베이스
    * 단일 컬럼의 크기는 최대 한 자릿 수 MB
    * 강력한 데이터 일관성을 보장
    * 디스크 IO가 최소화 
    * 가용성이 아주 높아야 하고 일부 장애를 감내
    * 증분 백업이 쉬워야함
  * 데이터 모델
    * user_id를 파티션 키로 사용하여 특정한 사용자의 데이터는 항상 같은 샤드에 보관하며 이를 보완하기 위해 파티션 키와 클러스터 키를 가져 모든 노드에 분산되도록 하며 같은 파티션 내에서 정렬
    * 특정 사용자의 모든 메일은 한 파티션 내에 있으므로 처리 순서가 보장
    * 읽지 않은 메일을 확인하는데 NoSQL을 이용하여 읽은 메일과 읽지 않은 메일을 구분하여 저장
  * 검색
    * 이메일 검색은 보통 이메일 제목이나 본문에 특정 키워가 포함되어있는지 찾음
    * 검색 기능을 제공하려면 이메일이 전송, 수신, 삭제될 때마다 색인 작업을 수행하여 검색 기능이 눌리면 수행
    * 쓰기 연산보다 읽기 연산이 많이 사용됨
    * ES를 주로 사용


실시간 게임 순위표
=======
* `실시간 게임 순위표 기능`
  * 1. 사용자는 경기에서 승리하면 포인트를 얻습니다.  
  * 2. 모든 플레이어가 순위표에 포함되어야 합니다.  
  * 3. 매달 새로운 토너먼트가 시작할 때마다 새로운 순위표를 만듭니다.  
  * 4. 상위 10명의 사용자와 특정 사용자의 순위를 순위표에 표시할 수 있어야 합니다.  
  * 5. DAU 500만 명, MAU 2,500만 명으로 가정합니다.  
  * 6. 플레이어는 하루 평균 10경기를 치룹니다.  
  * 7. 두 플레이어의 점수가 같을 경우 순위는 동일합니다.
  * 8. 순위표는 실시간이어야 합니다.  
* `실시간 게임 순위표 개략 설계`
![image](https://github.com/yoonje/dev-note/assets/38535571/3f572628-d133-493e-a8e8-052509f9af41)
  * API 설계
    * 사용자의 순위를 갱신 API
    * 순위표에서 상위 10명의 플레이어를 가져오는 API
    * 특정 사용자의 순위를 사져오는 API
  * 서비스 실행 순서
    * 플레이어(단말)은 게임 승리 시 게임 서비스를 호출
    * 게임 서비스는 플레이어의 점수를 계산하고 랭킹 서비스를 호출
    * 랭킹 서비스는 플레이어의 점수를 갱신
    * 클라이언트는 중간자 공격과 같은 보안상 문제로 점수 계산을 할 수 없으므로 반드시 서버에서 점수 계산을 수행
  * 관계형 데이터베이스
    * 사용자 수가 많지 않은 경우 관계형 데이터베이스를 사용하는 방법이 가능
    * 읽기 성능을 최적화 하기 위해서 색인과 LIMIT 절을 사용하는 방법을 사용할 수 있음
    * 수백만개의 레코드가 있는 경우 최적화를 하더라도 관계형 데이터베이스는 특정 사용자의 순위를 알기 위해서 전체를 다 읽어야하고 순위표 상단에 있지 않은 사용자의 순위를 간단히 찾을 수 없으며 실시간으로 점수가 변화하기 때문에 읽기 부하를 감당하기 어려울 수 있음
* `실시간 게임 순위표 상세 설계`
  * 정렬 집합
  ![image](https://github.com/yoonje/dev-note/assets/38535571/8364e942-3d76-4705-a6e7-15720f27a4ec)
    * 집합과 유사한 자료형으로 해시 테이블과 스킵 리스트라는 2가지 자료구조를 구성되어 있음
    * 순위표와 같은 기능에 매우 적합한 자료형
    * 해시테이블은 사용자의 점수를 저장하여 O(1)으로 검색하기 위해서 사용하고 스킵 리스트는 특정 점수를 딴 사용자들의 목록을 저장하여 O(logN)으로 검색하기 위해 사용
    * 정렬 집합은 사입과 갱신 연산을 할 때 모든 원소가 올바른 위치에 자동으로 배치되며 새 원소를 추가하거나 기존 원소를 검색하는 연산의 시간 복잡도가 낮아 관계형 데이터베이스보다 성능이 좋음
  * Redis
    * 메모리 기반 key-value 저장소로 메모리에서 동작하므로 빠른 읽기와 쓰기가 가능하고 정렬 집합 자료형을 제공하기 때문에 실시간 순위표 저장소로 적합
    * Redis 연산
      - ZADD (신규 플레이어 추가): https://redis.io/commands/zadd/
      - ZINCRBY (점수 갱신): https://redis.io/commands/zincrby/
      - ZRANGE (상위 10명 조회): https://redis.io/commands/zrange/
      - ZREVRANK (플레이어 랭킹 조회):  https://redis.io/commands/zrevrank/
    * Redis 영속성
      * Redis도 데이터를 디스크에 영구적으로 보관할 수 있지만 Redis 인스턴스가 재시작하는 등이 발생할 경우 준비 시간이 많이 걸리므로 Master - Slave로 구성하여 장애에 대응
    * Redis 샤딩
      * 고정 파티션
      ![image](https://github.com/yoonje/dev-note/assets/38535571/a6b11196-8882-4c79-9b53-dfa51a7ba27c)
        * 순위표에 등장하는 점수 범위에 따라 파티션을 나누는 방식으로 점수가 고르게 분포되는 경우 적합하지만 고르게 분포되지 않으면 핫스팟 샤드가 생기는 문제가 발생하는 방법
      * 해시 파티션
      ![image](https://github.com/yoonje/dev-note/assets/38535571/b83d9407-38ae-4c90-bd7a-2c6a3ad1b9ab)
        * 해시 연산을 통해 파티션을 나누는 방식으로 사용자들의 점수가 특정 대역에 과도하게 모이는 경우 샤딩하기에 적합한 방법
  * 분산 수집
  ![image](https://github.com/yoonje/dev-note/assets/38535571/e2521a5b-21a4-4267-9669-812e93f95567)
    * 상위 10명의 플레이어 정보는 모든 Node에서 상위 10명의 플레이어를 조회한 다음 애플리케이션 내에서 다시 정렬하는 분산-수집 접근법을 사용